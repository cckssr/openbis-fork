# Unique code of this Data Store Server. Not more than 40 characters.
data-store-server-code=standard
root-dir=targets
# The root directory of the data store
storeroot-dir=${root-dir}/store
# The directory for incoming files over rpc
rpc-incoming-dir=${root-dir}/incoming-rpc
core-plugins-folder=source/core-plugins
# The directory where the command queue file is located; defaults to storeroot-dir
commandqueue-dir=
# Port
port=8889
# Session timeout in minutes
session-timeout=30
# Set to 'false' for development/testing without deployed server. In this mode datastore will not use
# SSL when connecting to openbis. Otherwise all 'keystore' properties need to be set for SSL connection 
# (default when use-ssl property is not set so there is no need to specify it on production servers).
use-ssl=false
# Path, password and key password for SSL connection
#keystore.path = dist/etc/openBIS.keystore
#keystore.password = changeit
#keystore.key-password = changeit
# The check interval (in seconds)
check-interval=5
# The time-out for clean up work in the shutdown sequence (in seconds).
# Note that the maximal time for the shutdown sequence to complete can be as large
# as twice this time.
shutdown-timeout=2
# The minimum time (in seconds) of availability of the data stream since the time when user requested
# for the data stream url. If not specified default value (20 seconds) will be used.
# data-stream-timeout = 20
# If free disk space goes below value defined here, a notification email will be sent.
# Value must be specified in kilobytes (1048576 = 1024 * 1024 = 1GB). If no high water mark is
# specified or if value is negative, the system will not be watching.
highwater-mark=1048576
# If a data set is successfully registered it sends out an email to the registrator.
# If this property is not specified, no email is sent to the registrator. This property
# does not affect the mails which are sent, when the data set could not be registered.
notify-successful-registration=false
# The URL of the openBIS server
server-url=http://localhost:8888
# Time out for accessing openBIS server. Default value is 5 minutes.
server-timeout-in-minutes=10
# The location of the jars accessable to web start clients
webstart-jar-path=targets/dist/datastore_server/lib
# The username to use when contacting the openBIS server
username=etlserver
# The password to use when contacting the openBIS server
password=doesnotmatter
#
# CIFEX configuration: Only needed if data export should work without the user having to type in 
# his password. Note that in in order for this to work the CIFEX server needs to be configured to 
# allow calling setSessionUser() from the IP address of this data store server, see configuration 
# option allowed-ips-for-set-session-user in CIFEX's service.properties    
#
# The admin username to use when contacting the CIFEX server
cifex-admin-username=
# The admin password to use when contacting the CIFEX server
cifex-admin-password=
# The base URL for Web client access.
download-url=http://localhost:8889
# SMTP properties (must start with 'mail' to be considered).
mail.smtp.host=file://${root-dir}/email
mail.from=datastore_server@localhost
mail.smtp.user=
mail.smtp.password = 
# If this property is set a test e-mail will be sent to the specified address after DSS successfully started-up.
mail.test.address=test@localhost
# Email addresses of people to get notifications about problems in dataset registrations
mail.addresses.dropbox-errors=admin1@localhost,admin2@localhost
# ---------------------------------------------------------------------------
# Data sources
data-sources=path-info-db 
# Data source for pathinfo database
path-info-db.version-holder-class=ch.systemsx.cisd.openbis.dss.generic.shared.PathInfoDatabaseVersionHolder
path-info-db.databaseEngineCode=postgresql
path-info-db.basicDatabaseName=pathinfo
path-info-db.databaseKind=dev
path-info-db.scriptFolder=../server-original-data-store/source/sql
# ---------------------------------------------------------------------------
# Maximum number of retries if renaming failed.
# renaming.failure.max-retries = 12
# The number of milliseconds to wait before retrying to execute the renaming process.
# renaming.failure.millis-to-sleep = 5000
# Globally used separator character which separates entities in a data set file name
data-set-file-name-entity-separator=_
# The period of no write access that needs to pass before an incoming data item is considered
# complete and ready to be processed (in seconds) [default: 300]. 
# Valid only when auto-detection method is used to determine if an incoming data are ready to be processed.
quiet-period=3
# ---------------------------------------------------------------------------
# reporting and processing plugins configuration
# ---------------------------------------------------------------------------
# Comma separated names of reporting plugins. Each plugin should have configuration properties prefixed with its name.
# If name has 'default-' prefix it will be used by default in data set Data View.
reporting-plugins=demo-reporter, tsv-viewer, csv-viewer, hcs-viewer, hcs-viewer-tiff, default-hcs-viewer
# Label of the plugin which will be shown for the users.
demo-reporter.label=Show Dataset Size
# Comma separated list of dataset type codes which can be handled by this plugin.
# Wildcards are allowed, but the DSS server may need to be restarted if a new data set type is added to openBIS
demo-reporter.dataset-types=.*
# Plugin class specification (together with the list of packages this class belongs to).
demo-reporter.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoReportingPlugin
# The property file. Its content will be passed as a parameter to the plugin.
demo-reporter.properties-file=
tsv-viewer.label = TSV View 
tsv-viewer.dataset-types=TSV
tsv-viewer.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin
tsv-viewer.properties-file=
# Optional properties:
# - separator of values in the file, significant for TSV and CSV files; default: tab
#tsv-viewer.separator = ;
# - whether lines beginning with '#' should be ignored by the plugin; default: true
#tsv-viewer.ignore-comments = false
# - excel sheet name or index (0 based) used for the excel file (.xsl or .xslx); default: 0 (first sheet)
#tsv-viewer.excel-sheet = example_sheet_name
csv-viewer.label=CSV View 
csv-viewer.dataset-types=CSV
csv-viewer.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin
csv-viewer.separator=,
hcs-viewer.label=HCS PNG 	
hcs-viewer.dataset-types=HCS_IMAGE
hcs-viewer.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.GenericDssLinkReportingPlugin
hcs-viewer.download-url=${download-url}
hcs-viewer.data-set-regex=.*/PNG/.*\.jpg
hcs-viewer.data-set-path=original
hcs-viewer-tiff.label=HCS TIFF 	
hcs-viewer-tiff.dataset-types=HCS_IMAGE
hcs-viewer-tiff.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.GenericDssLinkReportingPlugin
hcs-viewer-tiff.download-url=${download-url}
hcs-viewer-tiff.data-set-regex=.*/TIFF/.*\.jpg
hcs-viewer-tiff.data-set-path=original
default-hcs-viewer.label=Default HCS View 	
default-hcs-viewer.dataset-types=HCS_IMAGE
default-hcs-viewer.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.GenericDssLinkReportingPlugin
default-hcs-viewer.download-url=${download-url}
default-hcs-viewer.data-set-regex=.*\.jpg
# Comma separated names of processing plugins. Each plugin should have configuration properties prefixed with its name.
processing-plugins=demo-processor
# The configuration of the processing plugin is the same as the reporting plugins configuration.
demo-processor.label=Demo Processing
# Wildcards are allowed, but the DSS server may need to be restarted if a new data set type is added to openBIS
demo-processor.dataset-types=HCS_.*, CONTAINER_.*
demo-processor.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoProcessingPlugin
demo-processor.properties-file=
# Data set validators used to accept or reject data sets to be registered.
# Comma separated list of validator definitions.
data-set-validators=validator
# Definition of data set validator 'validator'
validator.data-set-type=HCS_IMAGE
validator.path-patterns=**/*.txt
validator.columns=id, description, size
validator.id.header-pattern=ID
validator.id.mandatory=true
validator.id.order=1
validator.id.value-type=unique
validator.description.header-pattern=Description
validator.description.value-type=string
validator.description.value-pattern=.{0,100}
validator.size.header-pattern=A[0-9]+
validator.size.can-define-multiple-columns=true
validator.size.allow-empty-values=true
validator.size.value-type=numeric
validator.site.value-range=[0,Infinity)
# ---------------------------------------------------------------------------
# Comma separated names of processing threads. Each thread should have configuration properties prefixed with its name.
# E.g. 'code-extractor' property for the thread 'my-etl' should be specified as 'my-etl.code-extractor'
#inputs=main-thread, tsv-thread, csv-thread, simple-thread, hdf5-thread, dss-system-test-thread
#inputs=eln-lims-dropbox
#dss-rps.put-default=eln-lims-dropbox

inputs = default-dropbox
dss-rpc.put-default = default-dropbox

default-dropbox.incoming-dir = ${incoming-root-dir}/incoming-default
default-dropbox.incoming-data-completeness-condition = auto-detection
default-dropbox.top-level-data-set-handler = ch.systemsx.cisd.etlserver.registrator.api.v2.JavaTopLevelDataSetHandlerV2
default-dropbox.program-class = ch.systemsx.cisd.etlserver.registrator.DefaultDropbox
default-dropbox.storage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor

dss-rpc.put.CUSTOM-IMPORT=test-custom-import
# True if incoming directories should be created on server startup if they don't exist.
# Default - false (server will fail at startup if one of incoming directories doesn't exist). 
incoming-dir-create=true

# ---------------------------------------------------------------------------
# (optional) image overview plugins configuration
# ---------------------------------------------------------------------------
# Comma separated names of image overview plugins.
# Each plugin should have configuration properties prefixed with its name.
# Generic properties for each <plugin> include: 
#   <plugin>.class   - Fully qualified plugin class name (mandatory).
#   <plugin>.default - If true all data set types not handled by other plugins should be handled 
#                      by the plugin (default = false). 
#   <plugin>.dataset-types - Comma separated list of data set types handled by the plugin 
#                      (optional and ignored if default is true, otherwise mandatory). 
overview-plugins=default-overview, hcs-image-overview
default-overview.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoOverviewPlugin
default-overview.default=true
# Optional property specific to the plugin
default-overview.label=default plugin
hcs-image-overview.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoOverviewPlugin
hcs-image-overview.dataset-types=HCS_IMAGE
hcs-image-overview.label=plugin for HCS_IMAGE
# ---------------------------------------------------------------------------
# (optional) archiver configuration
# ---------------------------------------------------------------------------
# Configuration of an archiver task. All properties are prefixed with 'archiver.'.
# Archiver class specification (together with the list of packages this class belongs to).
#archiver.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.demo.DemoArchiver
archiver.class=ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.RsyncArchiver
# dectination of the archive (can be local or remote)
# local:
#archiver.destination = openbis:tmp/dest
# remote:
archiver.destination=/Users/openbis/dest
# indicates if data should be synchronized when local copy differs from one in archive (default: true)
archiver.synchronize-archive=true
archiver.batch-size-in-bytes=20000000
# ---------------------------------------------------------------------------
# maintenance plugins configuration
# ---------------------------------------------------------------------------
# Comma separated names of maintenance plugins.
# Each plugin should have configuration properties prefixed with its name.
# Mandatory properties for each <plugin> include: 
#   <plugin>.class - Fully qualified plugin class name
#   <plugin>.interval - The time between plugin executions (in seconds)
# Optional properties for each <plugin> include:
#   <plugin>.start - Time of the first execution (HH:mm)
#   <plugin>.execute-only-once - If true the task will be executed exactly once, 
#                                interval will be ignored. By default set to false.
maintenance-plugins=auto-archiver, path-info-deletion, post-registration
#maintenance-plugins = auto-archiver, path-info-feeding
# Performs automatic archivization of 'ACTIVE' data sets based on their properties
auto-archiver.class=ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask
# The time between subsequent archivizations (in seconds)
auto-archiver.interval=10
#  Time of the first execution (HH:mm)
auto-archiver.start=23:00
# following properties are optional
# only data sets of specified type will be archived  
#auto-archiver.data-set-type = UNKNOWN
# only data sets that are older than specified number of days will be archived (default = 0)  
#auto-archiver.older-than = 90
# fully qualified class name of a policy that additionally filters data sets to be filtered
#auto-archiver.policy.class = ch.systemsx.cisd.etlserver.plugins.DummyAutoArchiverPolicy
# use this policy to archive datasets in batches grouped by experiment
# auto-archiver.policy.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.ByExpermientPolicy
# the min-size in bytes, default is 0
#auto-archiver.policy.minimal-archive-size = 
# the max-size in bytes, default is 2^63-1. 
#auto-archiver.policy.maximal-archive-size = 
# Maintenance task (performed only once) to create paths of existing data sets in pathinfo database
path-info-feeding.class=ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask
path-info-feeding.execute-only-once=true
# Maintenance task for deleting entries in pathinfo database after deletion of data sets
path-info-deletion.class=ch.systemsx.cisd.etlserver.plugins.DeleteFromExternalDBMaintenanceTask
path-info-deletion.interval=120
path-info-deletion.data-source=path-info-db
path-info-deletion.data-set-perm-id=CODE
# Maintenance task for post registration of all paths of a freshly registered data set to be fed into pathinfo database
post-registration.class=ch.systemsx.cisd.etlserver.postregistration.PostRegistrationMaintenanceTask
post-registration.interval=30
post-registration.cleanup-tasks-folder=targets/cleanup-tasks
# The following date should the day when the DDS is started up the first time with PathInfoDatabaseFeedingTask.
# After PathInfoDatabaseFeedingTask has been performed it can be removed and the following line can be deleted.
#post-registration.ignore-data-sets-before-date = 2011-04-18
post-registration.last-seen-data-set-file=targets/last-seen-data-set
post-registration.post-registration-tasks=pathinfo-feeding
post-registration.pathinfo-feeding.class=ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask
post-registration.pathinfo-feeding.compute-checksum=true
post-registration.pathinfo-feeding.checksum-type=SHA1
jython-version=2.7
# Typical options to disable coping general, owner and group permissions
# rsync-options = --no-p --no-o --no-g
#
# Dropbox - ELN Plugin
#
default-incoming-share-id=1
default-incoming-share-minimum-free-space-in-gb=10
eln-lims.dss.drop-boxes.eln-lims-dropbox.discard-files-patterns=
eln-lims.dss.drop-boxes.eln-lims-dropbox.illegal-files-patterns=^desktop\\.ini, ^IconCache\\.db, ^thumbs\\.db, ^\\..*, ^.*'.*, ^.*~.*, ^.*\\$.*, ^.*%.*

# ftp.server.enable = true
# ftp.server.sftp-port = 2222
# ftp.resolver-dev-mode = true
# ftp.show-afs-data-sets = false